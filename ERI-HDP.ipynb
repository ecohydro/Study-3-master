{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Data preprocessing, NLP, topic models with HDP (Gensim), visualizaion with pyLDAvis (preliminary)\n",
    "# Result: implementations of topic models with high coherence scores from each library\n",
    "# Output: pyLDAvis, wordcloud, csvs (document-topic, topic-document...)\n",
    "\n",
    "# Load helper libraries\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Enable Gensim logging and warnings\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# Text pre-processing model (lemmatization) from spacy (en) and nltk\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn\n",
    "import wordcloud\n",
    "#import sklearn\n",
    "#import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3770.000000\n",
       "mean     1678.432626\n",
       "std       687.609548\n",
       "min       128.000000\n",
       "25%      1247.000000\n",
       "50%      1630.000000\n",
       "75%      1993.000000\n",
       "max      7083.000000\n",
       "Name: combined_len, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA Read in docs (corpus) from csv; report length of abstract text\n",
    "docs = pd.read_csv(\"data/ERI-combined-2009-2019.csv\")\n",
    "len(docs)\n",
    "\n",
    "# Concatenate title and abstract to new column for topic model\n",
    "docs['combined'] = docs['title'].astype(str) + ' ' + docs['abstract'].astype(str)\n",
    "docs.head()\n",
    "\n",
    "# Calculate lengths of combined title and abstract; add columns to dataframe for length of abstracts\n",
    "docs['title_len'] = docs['title'].apply(len)\n",
    "docs['abstract_len'] = docs['abstract'].apply(len)\n",
    "docs['combined_len'] = docs['combined'].apply(len)\n",
    "docs.head()\n",
    "\n",
    "# Calculate summary statistics for combined title and abstract lengths to determine text suitability\n",
    "docs.combined_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:4: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-4-7dc2a281053b>:4: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data] #remove new line characters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Streams and Urbanization Urbanization encompasses a diverse array of '\n",
      " 'watershed alterations that influence the physical, chemical, and biological '\n",
      " 'characteristics of streams. In this chapter, we summarize lessons learned '\n",
      " 'from the last half century of research on urban streams and provide a '\n",
      " 'critique of various mitigation strategies, including recent approaches that '\n",
      " 'explicitly address geomorphic processes. We focus first on the abiotic '\n",
      " 'conditions (primarily hydrologic and geomorphic) and their changes in '\n",
      " 'streams that accompany urbanization, recognizing that these changes may vary '\n",
      " 'with geomorphic context and climatic region. We then discuss technical '\n",
      " 'approaches and limitations to (1) mitigating water-quantity and '\n",
      " 'water-quality degradation through site design, riparian protection, and '\n",
      " 'structural stormwater-management strategies; and (2) restoring urban streams '\n",
      " 'in those watersheds where the economic, social, and political contexts can '\n",
      " 'support such activities.']\n"
     ]
    }
   ],
   "source": [
    "# DATA Title and abstract to list, removing new line characters, quotations\n",
    "#docs.head()\n",
    "data = docs['combined'].values.tolist()\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data] #remove new line characters\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data] #remove single quotes\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saralafia/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "# NLP Load stopwords from NLTK, extend default list with custom stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['data', 'study', 'project', 'research', 'collaborative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['streams', 'and', 'urbanization', 'urbanization', 'encompasses', 'diverse', 'array', 'of', 'watershed', 'alterations', 'that', 'influence', 'the', 'physical', 'chemical', 'and', 'biological', 'characteristics', 'of', 'streams', 'in', 'this', 'chapter', 'we', 'summarize', 'lessons', 'learned', 'from', 'the', 'last', 'half', 'century', 'of', 'research', 'on', 'urban', 'streams', 'and', 'provide', 'critique', 'of', 'various', 'mitigation', 'strategies', 'including', 'recent', 'approaches', 'that', 'explicitly', 'address', 'geomorphic', 'processes', 'we', 'focus', 'first', 'on', 'the', 'abiotic', 'conditions', 'primarily', 'hydrologic', 'and', 'geomorphic', 'and', 'their', 'changes', 'in', 'streams', 'that', 'accompany', 'urbanization', 'recognizing', 'that', 'these', 'changes', 'may', 'vary', 'with', 'geomorphic', 'context', 'and', 'climatic', 'region', 'we', 'then', 'discuss', 'technical', 'approaches', 'and', 'limitations', 'to', 'mitigating', 'water', 'quantity', 'and', 'water', 'quality', 'degradation', 'through', 'site', 'design', 'riparian', 'protection', 'and', 'structural', 'stormwater', 'management', 'strategies', 'and', 'restoring', 'urban', 'streams', 'in', 'those', 'watersheds', 'where', 'the', 'economic', 'social', 'and', 'political', 'contexts', 'can', 'support', 'such', 'activities']]\n"
     ]
    }
   ],
   "source": [
    "# NLP Tokenize each sentence into a list of lowercase words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True remove punctuation\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evidence', 'for', 'widespread', 'creep', 'on', 'the', 'flanks', 'of', 'the', 'sea', 'of', 'marmara', 'transform', 'basin', 'from', 'marine', 'geophysical', 'data', 'wave', 'fields', 'have', 'long', 'been', 'recognized', 'in', 'marine', 'sediments', 'on', 'the', 'flanks', 'of', 'basins', 'and', 'oceans', 'in', 'both', 'tectonically_active', 'and', 'inactive', 'environments', 'the', 'origin', 'of', 'waves', 'hereafter', 'called', 'undulations', 'is', 'controversial', 'competing', 'models', 'ascribe', 'them', 'to', 'depositional', 'processes', 'gravity', 'driven', 'downslope', 'creep', 'or', 'collapse', 'and', 'or', 'tectonic', 'shortening', 'here', 'we', 'analyze', 'pervasive', 'undulation', 'fields', 'identified', 'in', 'swath', 'bathymetry', 'and', 'new', 'high', 'resolution', 'multichannel_seismic', 'mcs', 'reflection', 'data', 'from', 'the', 'sea', 'of', 'marmara', 'turkey', 'although', 'they', 'exhibit', 'some', 'of', 'the', 'classical', 'features', 'of', 'sediment', 'waves', 'the', 'following', 'distinctive', 'characteristics', 'exclude', 'purely', 'depositional', 'origin', 'parallelism', 'between', 'the', 'crests', 'of', 'the', 'undulations', 'and', 'bathymetric', 'contours', 'over', 'wide_range', 'of', 'orientations', 'steep', 'flanks', 'of', 'the', 'undulations', 'up', 'to', 'and', 'increases', 'in', 'undulations', 'amplitude', 'with', 'depth', 'we', 'argue', 'that', 'the', 'undulations', 'are', 'folds', 'formed', 'by', 'gravity', 'driven', 'downslope', 'creep', 'that', 'have_been', 'augmented', 'by', 'depositional', 'processes', 'these', 'creep', 'folds', 'develop', 'over', 'long', 'time', 'periods', 'and', 'stand', 'in', 'contrast', 'to', 'geologically', 'instantaneous', 'collapse', 'stratigraphic', 'growth', 'on', 'the', 'upslope', 'limbs', 'indicates', 'that', 'deposition', 'contributes', 'to', 'the', 'formation', 'and', 'upslope', 'migration', 'of', 'the', 'folds', 'the', 'temporal', 'and', 'spatial', 'evolution', 'of', 'the', 'creep', 'folds', 'is', 'clearly', 'related', 'to', 'rapid', 'tilting', 'in', 'this', 'tectonically_active', 'transform', 'basin']\n"
     ]
    }
   ],
   "source": [
    "# NLP Bigram and trigram models (words frequently occurring together in the doc)\n",
    "bigram = gensim.models.Phrases(data_words, min_count=3, threshold=80) # higher threshold fewer phrases\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=80)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See bigram example (given a specific record)\n",
    "print(trigram_mod[bigram_mod[data_words[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Define functions for stopwords, bigrams, trigrams, and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stream', 'urbanization', 'urbanization', 'encompass', 'diverse', 'array', 'watershed', 'alteration', 'influence', 'physical', 'chemical', 'biological', 'characteristic', 'stream', 'chapter', 'summarize', 'lessons_learn', 'last', 'half', 'century', 'urban', 'stream', 'provide', 'critique', 'various', 'mitigation', 'strategy', 'include', 'recent', 'approach', 'explicitly', 'address', 'geomorphic', 'process', 'focus', 'first', 'abiotic', 'condition', 'primarily', 'hydrologic', 'geomorphic', 'change', 'stream', 'accompany', 'urbanization', 'recognize', 'change', 'may', 'vary', 'geomorphic', 'context', 'climatic', 'region', 'discuss', 'technical', 'approach', 'limitation', 'mitigate', 'water', 'quantity', 'water', 'quality', 'degradation', 'site', 'design', 'riparian', 'protection', 'structural', 'stormwater', 'management', 'strategy', 'restore', 'urban', 'stream', 'watershed', 'economic', 'social', 'political', 'context', 'support', 'activity']]\n"
     ]
    }
   ],
   "source": [
    "# NLP Remove stopwords, make bigrams, make trigrams, and lemmatize\n",
    "data_words_nostops = remove_stopwords(data_words) # stop words\n",
    "data_words_bigrams = make_bigrams(data_words_nostops) # bigrams\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams) # trigrams\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('abiotic', 1),\n",
       "  ('accompany', 1),\n",
       "  ('activity', 1),\n",
       "  ('address', 1),\n",
       "  ('alteration', 1),\n",
       "  ('approach', 2),\n",
       "  ('array', 1),\n",
       "  ('biological', 1),\n",
       "  ('century', 1),\n",
       "  ('change', 2),\n",
       "  ('chapter', 1),\n",
       "  ('characteristic', 1),\n",
       "  ('chemical', 1),\n",
       "  ('climatic', 1),\n",
       "  ('condition', 1),\n",
       "  ('context', 2),\n",
       "  ('critique', 1),\n",
       "  ('degradation', 1),\n",
       "  ('design', 1),\n",
       "  ('discuss', 1),\n",
       "  ('diverse', 1),\n",
       "  ('economic', 1),\n",
       "  ('encompass', 1),\n",
       "  ('explicitly', 1),\n",
       "  ('first', 1),\n",
       "  ('focus', 1),\n",
       "  ('geomorphic', 3),\n",
       "  ('half', 1),\n",
       "  ('hydrologic', 1),\n",
       "  ('include', 1),\n",
       "  ('influence', 1),\n",
       "  ('last', 1),\n",
       "  ('lessons_learn', 1),\n",
       "  ('limitation', 1),\n",
       "  ('management', 1),\n",
       "  ('may', 1),\n",
       "  ('mitigate', 1),\n",
       "  ('mitigation', 1),\n",
       "  ('physical', 1),\n",
       "  ('political', 1),\n",
       "  ('primarily', 1),\n",
       "  ('process', 1),\n",
       "  ('protection', 1),\n",
       "  ('provide', 1),\n",
       "  ('quality', 1),\n",
       "  ('quantity', 1),\n",
       "  ('recent', 1),\n",
       "  ('recognize', 1),\n",
       "  ('region', 1),\n",
       "  ('restore', 1),\n",
       "  ('riparian', 1),\n",
       "  ('site', 1),\n",
       "  ('social', 1),\n",
       "  ('stormwater', 1),\n",
       "  ('strategy', 2),\n",
       "  ('stream', 5),\n",
       "  ('structural', 1),\n",
       "  ('summarize', 1),\n",
       "  ('support', 1),\n",
       "  ('technical', 1),\n",
       "  ('urban', 2),\n",
       "  ('urbanization', 3),\n",
       "  ('various', 1),\n",
       "  ('vary', 1),\n",
       "  ('water', 2),\n",
       "  ('watershed', 2)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Create dictionary and corpus for topic modeling\n",
    "# Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View corpus based on term-frequency\n",
    "#print(corpus[:1])\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Save dictionary and corpus for future use - optional\n",
    "#import pickle\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#id2word.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4993562940458683\n"
     ]
    }
   ],
   "source": [
    "# Topic Model (HDP - Hierarchical Dirichlet Process)\n",
    "# Fully unsupervised; infers the number of topics through \"posterior inference\"\n",
    "# hdp_to_lda() Get corresponding alpha and beta values of a LDA almost equivalent to current HDP.\n",
    "# suggested_lda_model() .\n",
    "\n",
    "# hdpmodel = gensim.models.HdpModel(corpus=corpus, id2word=id2word)\n",
    "# hdp_topics = hdpmodel.get_topics() #Get the term topic matrix learned during inference\n",
    "# len(hdp_topics) #150\n",
    "# hdpmodel.show_topics(num_topics=150) #Give the most probable num_words words from num_topics topics\n",
    "# hdp_lda = hdpmodel.suggested_lda_model() #Get a trained ldamodel object which is closest to the current hdp model\n",
    "# hdp_coherence = CoherenceModel(model=hdpmodel,texts=texts,dictionary=id2word,coherence='c_v').get_coherence()\n",
    "print(hdp_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saralafia/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZATION (HDP --> LDA), Gensim - Show model in pyLDAvis (150 topics)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(hdp_lda, corpus, id2word)\n",
    "#pyLDAvis.save_html(vis, 'pyLDAvis/hdp-gensim-150.html') #saves pyLDAvis graphs as standalone webpage\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
