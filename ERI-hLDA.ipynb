{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Data preprocessing, NLP, topic models with hLDA with tomotopy\n",
    "# Result: implementations of topic model with low perplexity score\n",
    "# Output: tables with topic, keywords, children, parent, level (for visualization)\n",
    "# Resource: https://bab2min.github.io/tomotopy/v0.6.2/en/\n",
    "# TO DO: refine text processing (stopwords; n-grams, lemmatization); re-iterate w/beta-loss\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Streams and Urbanization Urbanization encompasses a diverse array of '\n",
      " 'watershed alterations that influence the physical, chemical, and biological '\n",
      " 'characteristics of streams. In this chapter, we summarize lessons learned '\n",
      " 'from the last half century of research on urban streams and provide a '\n",
      " 'critique of various mitigation strategies, including recent approaches that '\n",
      " 'explicitly address geomorphic processes. We focus first on the abiotic '\n",
      " 'conditions (primarily hydrologic and geomorphic) and their changes in '\n",
      " 'streams that accompany urbanization, recognizing that these changes may vary '\n",
      " 'with geomorphic context and climatic region. We then discuss technical '\n",
      " 'approaches and limitations to (1) mitigating water-quantity and '\n",
      " 'water-quality degradation through site design, riparian protection, and '\n",
      " 'structural stormwater-management strategies; and (2) restoring urban streams '\n",
      " 'in those watersheds where the economic, social, and political contexts can '\n",
      " 'support such activities.']\n"
     ]
    }
   ],
   "source": [
    "# DATA Read in docs (corpus) from csv; concatenate title and abstract to new column for topic model\n",
    "docs = pd.read_csv(\"data/ERI-combined-2009-2019.csv\")\n",
    "docs['combined'] = docs['title'].astype(str) + ' ' + docs['abstract'].astype(str)\n",
    "data = docs['combined'].values.tolist()\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data] #remove new line characters\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data] #remove single quotes\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3770"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Lemmatize and remove stopwords; generate corpus\n",
    "# To do: Generate ngrams (NLTK) - https://snippets.aktagon.com/snippets/619-how-to-generate-n-grams-with-python-and-nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = WordNetLemmatizer() # both reduce words to same form; stemming is algorithmic, lemmatization uses corpus only\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['data', 'study', 'project', 'research', 'collaborative', 'use', 'include', 'result', 'increase', 'high', 'low', 'large', 'include', 'based']) # extends defaults with custom words\n",
    "\n",
    "corpus = tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer(stemmer=stemmer.lemmatize), \n",
    "                         stopwords=lambda x: len(x) <= 2 or x in stopwords)\n",
    "corpus.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tLog-likelihood: -9.156869662799986\n",
      "Iteration: 10\tLog-likelihood: -8.776159236498593\n",
      "Iteration: 20\tLog-likelihood: -8.596020156789908\n",
      "Iteration: 30\tLog-likelihood: -8.491875142965466\n",
      "Iteration: 40\tLog-likelihood: -8.421475270606024\n",
      "Iteration: 50\tLog-likelihood: -8.376578551009924\n",
      "Iteration: 60\tLog-likelihood: -8.34440432527948\n",
      "Iteration: 70\tLog-likelihood: -8.321666336599915\n",
      "Iteration: 80\tLog-likelihood: -8.305640856965953\n",
      "Iteration: 90\tLog-likelihood: -8.2931085972526\n",
      "Total number of topics:  1256\n",
      "Total number of live topics:  1077\n",
      "Depth:  9\n",
      "Perplexity:  3996.237599170139\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELING (hLDA) - testing hierarchical LDA topic model with tomotopy\n",
    "h_mdl = tp.HLDAModel(depth=9,corpus=corpus,seed=1)\n",
    "    \n",
    "for i in range(0, 100, 10): #Train the model using Gibbs-sampling\n",
    "    h_mdl.train(10)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, h_mdl.ll_per_word))\n",
    "\n",
    "print(\"Total number of topics: \", h_mdl.k)\n",
    "print(\"Total number of live topics: \", h_mdl.live_k)\n",
    "print(\"Depth: \", h_mdl.depth)\n",
    "print(\"Perplexity: \", h_mdl.perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Docs</th>\n",
       "      <th>Children</th>\n",
       "      <th>Parent</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0,)</td>\n",
       "      <td>([(model, 0.015386220999062061), (system, 0.00...</td>\n",
       "      <td>(3770,)</td>\n",
       "      <td>([12, 11, 10, 9, 8],)</td>\n",
       "      <td>(-1,)</td>\n",
       "      <td>(0,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1,)</td>\n",
       "      <td>([(model, 2.9001477741985582e-05), (change, 2....</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>([],)</td>\n",
       "      <td>(-1,)</td>\n",
       "      <td>(-1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2,)</td>\n",
       "      <td>([(model, 2.9001477741985582e-05), (change, 2....</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>([],)</td>\n",
       "      <td>(-1,)</td>\n",
       "      <td>(-1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3,)</td>\n",
       "      <td>([(model, 2.9001477741985582e-05), (change, 2....</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>([],)</td>\n",
       "      <td>(-1,)</td>\n",
       "      <td>(-1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(4,)</td>\n",
       "      <td>([(model, 2.9001477741985582e-05), (change, 2....</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>([],)</td>\n",
       "      <td>(-1,)</td>\n",
       "      <td>(-1,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic                                           Keywords     Docs  \\\n",
       "0  (0,)  ([(model, 0.015386220999062061), (system, 0.00...  (3770,)   \n",
       "1  (1,)  ([(model, 2.9001477741985582e-05), (change, 2....     (0,)   \n",
       "2  (2,)  ([(model, 2.9001477741985582e-05), (change, 2....     (0,)   \n",
       "3  (3,)  ([(model, 2.9001477741985582e-05), (change, 2....     (0,)   \n",
       "4  (4,)  ([(model, 2.9001477741985582e-05), (change, 2....     (0,)   \n",
       "\n",
       "                Children Parent  Level  \n",
       "0  ([12, 11, 10, 9, 8],)  (-1,)   (0,)  \n",
       "1                  ([],)  (-1,)  (-1,)  \n",
       "2                  ([],)  (-1,)  (-1,)  \n",
       "3                  ([],)  (-1,)  (-1,)  \n",
       "4                  ([],)  (-1,)  (-1,)  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOPIC MODELING (hLDA) - explore the topics (children, parents, depth, number of topics per level) as table\n",
    "rows = []\n",
    "for k in range(h_mdl.k):\n",
    "    topic = k,\n",
    "    keyword = h_mdl.get_topic_words(k, top_n=10),\n",
    "    docs = h_mdl.num_docs_of_topic(k),\n",
    "    children = h_mdl.children_topics(k),\n",
    "    parent = h_mdl.parent_topic(k),\n",
    "    level = h_mdl.level(k),\n",
    "    rows.append([topic, keyword, docs, children, parent, level])\n",
    "\n",
    "topics_df = pd.DataFrame(rows, columns=[\"Topic\", \"Keywords\", \"Docs\", \"Children\", \"Parent\", \"Level\"])\n",
    "topics_df.to_csv('outputs/hLDA-9-level.csv')\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
