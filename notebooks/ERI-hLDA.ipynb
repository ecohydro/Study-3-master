{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Data preprocessing, NLP, topic models and perplexity scores with hLDA (tomotopy)\n",
    "# Output: tables per level (topic, keywords, children, parent, level)\n",
    "# Resource: https://bab2min.github.io/tomotopy/v0.6.2/en/\n",
    "# To do: refine text processing; extract document-topic distributions at each level; pivot table\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Streams and Urbanization Urbanization encompasses a diverse array of '\n",
      " 'watershed alterations that influence the physical, chemical, and biological '\n",
      " 'characteristics of streams. In this chapter, we summarize lessons learned '\n",
      " 'from the last half century of research on urban streams and provide a '\n",
      " 'critique of various mitigation strategies, including recent approaches that '\n",
      " 'explicitly address geomorphic processes. We focus first on the abiotic '\n",
      " 'conditions (primarily hydrologic and geomorphic) and their changes in '\n",
      " 'streams that accompany urbanization, recognizing that these changes may vary '\n",
      " 'with geomorphic context and climatic region. We then discuss technical '\n",
      " 'approaches and limitations to (1) mitigating water-quantity and '\n",
      " 'water-quality degradation through site design, riparian protection, and '\n",
      " 'structural stormwater-management strategies; and (2) restoring urban streams '\n",
      " 'in those watersheds where the economic, social, and political contexts can '\n",
      " 'support such activities.']\n"
     ]
    }
   ],
   "source": [
    "# DATA Read in docs (corpus) from csv; concatenate title and abstract to new column for topic model\n",
    "docs = pd.read_csv(\"data/ERI-combined-2009-2019.csv\")\n",
    "docs['combined'] = docs['title'].astype(str) + ' ' + docs['abstract'].astype(str)\n",
    "data = docs['combined'].values.tolist()\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data] #remove new line characters\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data] #remove single quotes\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3770"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP Lemmatize and remove stopwords; generate corpus\n",
    "# TO DO: refine text processing (https://snippets.aktagon.com/snippets/619-how-to-generate-n-grams-with-python-and-nltk)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = WordNetLemmatizer() # both reduce words to same form; stemming is algorithmic, lemmatization uses corpus only\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['data', 'study', 'project', 'research', 'collaborative', 'use', 'include', 'result', 'increase', 'high', 'low', 'large', 'include', 'based']) # extends defaults with custom words\n",
    "\n",
    "corpus = tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer(stemmer=stemmer.lemmatize), \n",
    "                         stopwords=lambda x: len(x) <= 2 or x in stopwords)\n",
    "corpus.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\tLog-likelihood: -8.874829779208273\n",
      "Iteration: 10\tLog-likelihood: -8.549331638287489\n",
      "Iteration: 20\tLog-likelihood: -8.420679203788414\n",
      "Iteration: 30\tLog-likelihood: -8.34197303038161\n",
      "Iteration: 40\tLog-likelihood: -8.289698554923566\n",
      "Iteration: 50\tLog-likelihood: -8.249507336026864\n",
      "Iteration: 60\tLog-likelihood: -8.226158093088047\n",
      "Iteration: 70\tLog-likelihood: -8.212455045505013\n",
      "Iteration: 80\tLog-likelihood: -8.20045246715981\n",
      "Iteration: 90\tLog-likelihood: -8.195003170291228\n",
      "Number of topics:  560\n",
      "Number of live topics:  522\n",
      "Number of documents:  3770\n",
      "Model perplexity:  3622.8024772368335\n"
     ]
    }
   ],
   "source": [
    "# TOPIC MODELING (hLDA) - testing hierarchical LDA topic model with tomotopy\n",
    "h_mdl = tp.HLDAModel(depth=4,corpus=corpus,seed=1)\n",
    "    \n",
    "for i in range(0, 100, 10): #Train the model using Gibbs-sampling\n",
    "    h_mdl.train(10)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, h_mdl.ll_per_word))\n",
    "\n",
    "print(\"Number of topics: \", h_mdl.k)\n",
    "print(\"Number of live topics: \", h_mdl.live_k)\n",
    "print(\"Number of documents: \", len(h_mdl.docs))\n",
    "print(\"Model perplexity: \", h_mdl.perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT (hLDA) - Explore the topics (children, parents, depth, number of topics per level) as csv\n",
    "rows = []\n",
    "for k in range(h_mdl.k):\n",
    "    topic = k,\n",
    "    keyword = h_mdl.get_topic_words(k, top_n=10),\n",
    "    num_docs = h_mdl.num_docs_of_topic(k),\n",
    "    children = h_mdl.children_topics(k),\n",
    "    parent = h_mdl.parent_topic(k),\n",
    "    level = h_mdl.level(k),\n",
    "    rows.append([topic, keyword, num_docs, children, parent, level])\n",
    "\n",
    "topics_df = pd.DataFrame(rows, columns=[\"Topic\", \"Keywords\", \"Num_Docs\", \"Children\", \"Parent\", \"Level\"])\n",
    "#topics_df.to_csv('outputs/hLDA/hLDA-4-level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT (hLDA) - Explore the documents as csv\n",
    "# TO DO: extract document-topic distributions (https://github.com/bab2min/tomotopy/issues/44)\n",
    "#len(h_mdl.docs)\n",
    "#doc_ins = h_mdl.docs[89] #document instance\n",
    "#print(doc_ins.weight)\n",
    "#print(doc_ins.words)\n",
    "#print(doc_ins.get_topic_dist()) #returning levels\n",
    "#print(doc_ins.get_topics(top_n=100))\n",
    "#print(doc_ins.get_words(top_n=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Level</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Level    1     2      3\n",
       "Topic                  \n",
       "8      0.0   NaN    NaN\n",
       "9      0.0   NaN    NaN\n",
       "10     0.0   NaN    NaN\n",
       "11     0.0   NaN    NaN\n",
       "12     0.0   NaN    NaN\n",
       "13     0.0   NaN    NaN\n",
       "14     0.0   NaN    NaN\n",
       "15     0.0   NaN    NaN\n",
       "16     NaN   8.0    NaN\n",
       "17     NaN  10.0    NaN\n",
       "18     NaN   8.0    NaN\n",
       "19     NaN  10.0    NaN\n",
       "20     NaN  11.0    NaN\n",
       "21     NaN  11.0    NaN\n",
       "22     NaN   8.0    NaN\n",
       "23     NaN  12.0    NaN\n",
       "24     NaN   NaN   16.0\n",
       "25     NaN   NaN   16.0\n",
       "26     NaN   NaN  257.0\n",
       "27     NaN   NaN   21.0\n",
       "28     NaN   NaN   16.0\n",
       "29     NaN   NaN   19.0\n",
       "30     NaN   NaN   16.0\n",
       "31     NaN   NaN   20.0\n",
       "32     NaN   NaN   21.0\n",
       "33     NaN   NaN   16.0\n",
       "34     NaN   NaN   20.0\n",
       "35     NaN   NaN   22.0\n",
       "36     NaN   NaN  160.0\n",
       "37     NaN   NaN   16.0"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VISUALIZATION - Transform topic table to hierarchical data structure for treemap\n",
    "# TO DO: pivot table\n",
    "h_data = pd.read_csv(\"outputs/hLDA/hLDA-4-level.csv\")\n",
    "level_index = h_data[h_data['Parent'] == -1 ].index #drop topics with parent -1 (top level)\n",
    "h_data.drop(level_index, inplace=True) #preserve original topic numbers\n",
    "pivoted_h = h_data.pivot(index='Topic', columns='Level', values='Parent')\n",
    "pivoted_h.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
